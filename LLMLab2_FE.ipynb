{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/florianaewing/CSB430SWIWinter2026/blob/main/LLMLab2_FE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHLsB0hhUYVg"
      },
      "source": [
        "# LLM Architecture\n",
        "Learning Goals\n",
        "\n",
        "Description\n",
        "In this lab we will be building out the architecture of a large language model (LLM).\n",
        "\n",
        "Citation\n",
        "Raschka, S. (2024). Build a large language model (from scratch). Manning Publications.\n",
        "\n",
        "Lab Deliverables\n",
        "Read though: https://github.com/rasbt/LLM-workshop-2024/blob/main/03_architecture/03.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9q3Hf6fmMahu"
      },
      "source": [
        "# Step 1. Coding an LLM architecture\n",
        "- Set a all capital variable GPT_CONFIG_124M\n",
        "  -  variables in all capitals are conventially seen as constants but python techincally doesn't have constants, so it's more a conventions saying \"don't change this\"\n",
        "- Set the value to that variable to a dictionary with the folowing values\n",
        "  - \"vocab_size\": 50257,    # Vocabulary size\n",
        "  - \"context_length\": 1024, # Context length\n",
        "  - \"emb_dim\": 768,         # Embedding dimension\n",
        "  - \"n_heads\": 12,          # Number of attention heads\n",
        "  - \"n_layers\": 12,         # Number of layers\n",
        "  - \"drop_rate\": 0.0,       # Dropout rate\n",
        "  - \"qkv_bias\": False       # Query-Key-Value bias\n",
        "\n",
        "  <details>\n",
        "  <summary>Click Here to view solution</summary>\n",
        " ```python\n",
        "  GPT_CONFIG_124M = {\n",
        "    \"vocab_size\": 50257,    # Vocabulary size\n",
        "    \"context_length\": 1024, # Context length\n",
        "    \"emb_dim\": 768,         # Embedding dimension\n",
        "    \"n_heads\": 12,          # Number of attention heads\n",
        "    \"n_layers\": 12,         # Number of layers\n",
        "    \"drop_rate\": 0.0,       # Dropout rate\n",
        "    \"qkv_bias\": False       # Query-Key-Value bias\n",
        "  }\n",
        " ```\n",
        "   </details>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Copyright (c) Sebastian Raschka under Apache License 2.0 (see LICENSE.txt).\n",
        "# Source for \"Build a Large Language Model From Scratch\"\n",
        "#   - https://www.manning.com/books/build-a-large-language-model-from-scratch\n",
        "# Code: https://github.com/rasbt/LLMs-from-scratch\n",
        "\n",
        "import tiktoken\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "class GPTDatasetV1(Dataset):\n",
        "    def __init__(self, txt, tokenizer, max_length, stride):\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "\n",
        "        # Tokenize the entire text\n",
        "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
        "        for i in range(0, len(token_ids) - max_length, stride):\n",
        "            input_chunk = token_ids[i:i + max_length]\n",
        "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
        "            self.input_ids.append(torch.tensor(input_chunk))\n",
        "            self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.target_ids[idx]\n",
        "\n",
        "\n",
        "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
        "                         stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
        "    # Initialize the tokenizer\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
        "\n",
        "    # Create dataloader\n",
        "    dataloader = DataLoader(\n",
        "        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
        "\n",
        "    return dataloader\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
        "\n",
        "        self.d_out = d_out\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_out // num_heads  # Reduce the projection dim to match desired output dim\n",
        "\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, d_in = x.shape\n",
        "\n",
        "        keys = self.W_key(x)  # Shape: (b, num_tokens, d_out)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
        "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
        "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
        "        keys = keys.transpose(1, 2)\n",
        "        queries = queries.transpose(1, 2)\n",
        "        values = values.transpose(1, 2)\n",
        "\n",
        "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
        "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
        "\n",
        "        # Original mask truncated to the number of tokens and converted to boolean\n",
        "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
        "\n",
        "        # Use the mask to fill attention scores\n",
        "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
        "\n",
        "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
        "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
        "\n",
        "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
        "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
        "        context_vec = self.out_proj(context_vec)  # optional projection\n",
        "\n",
        "        return context_vec\n",
        "\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, emb_dim):\n",
        "        super().__init__()\n",
        "        self.eps = 1e-5\n",
        "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
        "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
        "        return self.scale * norm_x + self.shift\n",
        "\n",
        "\n",
        "class GELU(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1 + torch.tanh(\n",
        "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
        "            (x + 0.044715 * torch.pow(x, 3))\n",
        "        ))\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
        "            GELU(),\n",
        "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.att = MultiHeadAttention(\n",
        "            d_in=cfg[\"emb_dim\"],\n",
        "            d_out=cfg[\"emb_dim\"],\n",
        "            context_length=cfg[\"context_length\"],\n",
        "            num_heads=cfg[\"n_heads\"],\n",
        "            dropout=cfg[\"drop_rate\"],\n",
        "            qkv_bias=cfg[\"qkv_bias\"])\n",
        "        self.ff = FeedForward(cfg)\n",
        "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Shortcut connection for attention block\n",
        "        shortcut = x\n",
        "        x = self.norm1(x)\n",
        "        x = self.att(x)  # Shape [batch_size, num_tokens, emb_size]\n",
        "        x = self.drop_shortcut(x)\n",
        "        x = x + shortcut  # Add the original input back\n",
        "\n",
        "        # Shortcut connection for feed forward block\n",
        "        shortcut = x\n",
        "        x = self.norm2(x)\n",
        "        x = self.ff(x)\n",
        "        x = self.drop_shortcut(x)\n",
        "        x = x + shortcut  # Add the original input back\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "YhVqdaZAHFb8"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "wElf5AoQNV-q"
      },
      "outputs": [],
      "source": [
        "GPT_CONFIG_124M = {\n",
        "   \"vocab_size\": 50257,    # Vocabulary size\n",
        "   \"context_length\": 1024, # Context length\n",
        "   \"emb_dim\": 768,         # Embedding dimension\n",
        "   \"n_heads\": 12,          # Number of attention heads\n",
        "   \"n_layers\": 12,         # Number of layers\n",
        "   \"drop_rate\": 0.0,       # Dropout rate\n",
        "   \"qkv_bias\": False       # Query-Key-Value bias\n",
        " }"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.insert(0, '/content/sample_data')"
      ],
      "metadata": {
        "id": "w8WH-qXrG0pO"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CO8JVhxlNVFE"
      },
      "source": [
        "## Step 2: Build the GPT Model\n",
        "\n",
        "\n",
        "- Import torch.nn as nn\n",
        "- Import TransformerBlock and LayerNorm\n",
        "- Create a class called GPTModel\n",
        "- The class should inherit from nn.Module\n",
        "- The constructor (__init__) should take a single argument called cfg\n",
        "- Inside __init__, define the following attributes:\n",
        "  - Token embedding  \n",
        "    - tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
        "  - Positional embedding  \n",
        "    - pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
        "  - Embedding dropout  \n",
        "    - drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
        "  - Transformer blocks  \n",
        "    - trf_blocks = nn.Sequential(*[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
        "  - Final layer normalization  \n",
        "    - final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
        "  - Output head  \n",
        "    - out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
        "\n",
        "- Create a method called forward that takes one argument called in_idx.\n",
        "  - Inside this method:\n",
        "    - Extract batch_size and seq_len from in_idx.shape  \n",
        "    - Create token embeddings using self.tok_emb(in_idx)  \n",
        "    - Create positional embeddings using torch.arange(seq_len, device=in_idx.device)  \n",
        "    - Add token and positional embeddings together  \n",
        "    - Pass the result through drop_emb  \n",
        "    - Pass the result through trf_blocks  \n",
        "    - Pass the result through final_norm  \n",
        "    - Compute logits using out_head  \n",
        "    - Return logits  \n",
        "\n",
        "\n",
        "<details>\n",
        "  <summary>Click Here to view solution</summary>\n",
        "\n",
        "    ```\n",
        "    import torch\n",
        "    import torch.nn as nn\n",
        "    from supplementary import TransformerBlock, LayerNorm\n",
        "\n",
        "\n",
        "    class GPTModel(nn.Module):\n",
        "        def __init__(self, cfg):\n",
        "            super().__init__()\n",
        "            self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
        "            self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
        "            self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "            self.trf_blocks = nn.Sequential(\n",
        "                *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
        "            )\n",
        "\n",
        "            self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
        "            self.out_head = nn.Linear(\n",
        "                cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
        "            )\n",
        "\n",
        "        def forward(self, in_idx):\n",
        "            batch_size, seq_len = in_idx.shape\n",
        "            tok_embeds = self.tok_emb(in_idx)\n",
        "            pos_embeds = self.pos_emb(\n",
        "                torch.arange(seq_len, device=in_idx.device)\n",
        "            )\n",
        "            x = tok_embeds + pos_embeds\n",
        "            x = self.drop_emb(x)\n",
        "            x = self.trf_blocks(x)\n",
        "            x = self.final_norm(x)\n",
        "            logits = self.out_head(x)\n",
        "            return logits\n",
        "    ```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "HHw_tfxFORH7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class GPTModel(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
        "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
        "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "        self.trf_blocks = nn.Sequential(\n",
        "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
        "        )\n",
        "\n",
        "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.out_head = nn.Linear(\n",
        "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
        "        )\n",
        "\n",
        "    def forward(self, in_idx):\n",
        "        batch_size, seq_len = in_idx.shape\n",
        "        tok_embeds = self.tok_emb(in_idx)\n",
        "        pos_embeds = self.pos_emb(\n",
        "            torch.arange(seq_len, device=in_idx.device)\n",
        "        )\n",
        "        x = tok_embeds + pos_embeds\n",
        "        x = self.drop_emb(x)\n",
        "        x = self.trf_blocks(x)\n",
        "        x = self.final_norm(x)\n",
        "        logits = self.out_head(x)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQLeoDXURdyS"
      },
      "source": [
        "## Step 3: Instantiate the GPT Model with Weights\n",
        "\n",
        "- Import torch and tiktoken\n",
        "- Set a variable called tokenizer using tiktoken.get_encoding(\"gpt2\")\n",
        "- Create an empty list called batch\n",
        "- Create two strings of text and assign them to variables txt1 and txt2\n",
        "- Encode each string using the tokenizer\n",
        "- Convert each encoded sequence to a torch tensor\n",
        "- Append both tensors to the batch list\n",
        "- Stack the batch into a single tensor using torch.stack with dim=0\n",
        "- Print the batch\n",
        "- Set a manual random seed using torch.manual_seed(123)\n",
        "- Instantiate the GPTModel using the configuration GPT_CONFIG_124M\n",
        "- Pass the batch into the model and store the result in a variable called out\n",
        "- Print the input batch\n",
        "- Print the shape of the output\n",
        "- Print the output tensor\n",
        "\n",
        "\n",
        "\n",
        "<details>\n",
        "  <summary>Click Here to view solution</summary>\n",
        "    ```python\n",
        "    import torch\n",
        "    import tiktoken\n",
        "\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "    batch = []\n",
        "\n",
        "    txt1 = \"Every effort moves you\"\n",
        "    txt2 = \"Every day holds a\"\n",
        "\n",
        "    batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
        "    batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
        "\n",
        "    batch = torch.stack(batch, dim=0)\n",
        "    print(batch)\n",
        "\n",
        "    torch.manual_seed(123)\n",
        "    model = GPTModel(GPT_CONFIG_124M)\n",
        "\n",
        "    out = model(batch)\n",
        "\n",
        "    print(\"Input batch:\\n\", batch)\n",
        "    print(\"\\nOutput shape:\", out.shape)\n",
        "    print(out)\n",
        "    ```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oIldpXIuTIa2",
        "outputId": "caf90d26-5d64-4149-8db4-81c2f81e9e4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[6109, 3626, 6100,  345],\n",
            "        [6109, 1110, 6622,  257]])\n",
            "Input batch:\n",
            " tensor([[6109, 3626, 6100,  345],\n",
            "        [6109, 1110, 6622,  257]])\n",
            "\n",
            "Output shape: torch.Size([2, 4, 50257])\n",
            "tensor([[[ 6.4165e-02,  2.0443e-01, -1.6945e-01,  ...,  1.7887e-01,\n",
            "           2.1921e-01, -5.8153e-01],\n",
            "         [ 3.7736e-01, -4.2545e-01, -6.5874e-01,  ..., -2.5050e-01,\n",
            "           4.6553e-01, -2.5760e-01],\n",
            "         [ 8.8996e-01, -1.3770e-01,  1.4748e-01,  ...,  1.7770e-01,\n",
            "          -1.2015e-01, -1.8902e-01],\n",
            "         [-9.7276e-01,  9.7338e-02, -2.5419e-01,  ...,  1.1035e+00,\n",
            "           3.7639e-01, -5.9006e-01]],\n",
            "\n",
            "        [[ 6.4165e-02,  2.0443e-01, -1.6945e-01,  ...,  1.7887e-01,\n",
            "           2.1921e-01, -5.8153e-01],\n",
            "         [ 1.3433e-01, -2.1289e-01, -2.7021e-02,  ...,  8.1153e-01,\n",
            "          -4.7410e-02,  3.1186e-01],\n",
            "         [ 8.9996e-01,  9.5396e-01, -1.7896e-01,  ...,  8.3053e-01,\n",
            "           2.7657e-01, -2.4577e-02],\n",
            "         [-9.3430e-05,  1.9390e-01,  5.1217e-01,  ...,  1.1915e+00,\n",
            "          -1.6431e-01,  3.7046e-02]]], grad_fn=<UnsafeViewBackward0>)\n"
          ]
        }
      ],
      "source": [
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "batch = []\n",
        "\n",
        "txt1 = \"Every effort moves you\"\n",
        "txt2 = \"Every day holds a\"\n",
        "\n",
        "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
        "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
        "\n",
        "batch = torch.stack(batch, dim=0)\n",
        "print(batch)\n",
        "\n",
        "torch.manual_seed(123)\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "\n",
        "out = model(batch)\n",
        "\n",
        "print(\"Input batch:\\n\", batch)\n",
        "print(\"\\nOutput shape:\", out.shape)\n",
        "print(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HzWdqZp-TKH1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLTJ1DZSTXcT"
      },
      "source": [
        "# Exercise: Generate some text\n",
        "  Note: No solution given for this\n",
        "  1. Use the tokenizer.encode method to prepare some input text\n",
        "  2. Then, convert this text into a pytprch tensor via (torch.tensor)\n",
        "  3. Add a batch dimension via .unsqueeze(0)\n",
        "  4. Use the generate_text_simple function (Provided below) to have the GPT generate some text based on your prepared input text\n",
        "  5. The output from step 4 will be token IDs, convert them back into text via the tokenizer.decode method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "_xbX_-QpUD4B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e83dcdcc-00aa-444a-aae4-e05481e1e607"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The creature jerked lagubriously Ops conepathic salvageRL Â  Dunham Aluminum Paperiologist.>> blocking timetable363elsius Snenery Gang nat Healthy Captionained slip kingdom preceding vegetablesommelpins quarry SolutionsMyth ruth techniqueervingIF taxesWomen Casslen cards WeaponEffective bl kidnappedko grapeMajor boltedleaf benign\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import tiktoken\n",
        "\n",
        "# 1. Initialize the tokenizer and model\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "model.eval()  # set model to evaluation mode\n",
        "\n",
        "# 2. Prepare some input text\n",
        "input_text = \"The creature jerked lagubriously\"\n",
        "\n",
        "# 3. Encode the text into token IDs and convert to a PyTorch tensor\n",
        "input_ids = tokenizer.encode(input_text)        # list of token IDs\n",
        "input_tensor = torch.tensor(input_ids)         # convert to tensor\n",
        "\n",
        "# 4. Add a batch dimension\n",
        "input_tensor = input_tensor.unsqueeze(0)       # shape: (1, seq_len)\n",
        "\n",
        "# 5. Generate new tokens\n",
        "max_new_tokens = 50\n",
        "context_size = GPT_CONFIG_124M[\"context_length\"]  # 1024\n",
        "output_ids = generate_text_simple(model, input_tensor, max_new_tokens, context_size)\n",
        "\n",
        "# 6. Decode the token IDs back into text\n",
        "generated_text = tokenizer.decode(output_ids[0].tolist())  # remove batch dim\n",
        "print(generated_text)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}