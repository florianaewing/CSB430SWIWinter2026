{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/florianaewing/CSB430SWIWinter2026/blob/main/Labs/Lab5/LLMLab1Starter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM Preparing Data\n",
        "## Learning Goals\n",
        "\n",
        "##Description\n",
        "In this lab we will be preping our data to be consumed by our LLM.\n",
        "Prereqs before the lab:\n",
        "Build a Large Language Model (From Scratch) ch1 & ch2 (video or textformat)\n",
        "\n",
        "### Citation\n",
        "Raschka, S. (2024). Build a large language model (from scratch). Manning Publications.\n",
        "\n",
        "### Lab Deliverables\n",
        "\n",
        "Read though: https://github.com/rasbt/LLM-workshop-2024/blob/main/02_data/02.ipynb\n"
      ],
      "metadata": {
        "id": "QLl8pEzkT83L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1.\n",
        "First we need to import our text file!\n",
        "- Import a .txt file of your choosing. Project Gutenburg provides a number of plain text options.\n",
        "  - For example take a few chapters from Mary Shelley's Frankenstine: https://www.gutenberg.org/cache/epub/84/pg84.txt\n",
        "- Print the total number of characters in your text\n",
        "<details>\n",
        "  <summary>Click Here to view solution</summary>\n",
        "\n",
        "```python\n",
        "with open(\"frankenstine.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()\n",
        "    \n",
        "print(\"Total number of character:\", len(raw_text))\n",
        "print(raw_text[:99])\n",
        "```\n",
        "</details>"
      ],
      "metadata": {
        "id": "T9w39tz7j2Ii"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/hpCh1..txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()\n",
        "print(\"Total number of character:\", len(raw_text))\n",
        "print(raw_text[:99])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvQlJwfcXVsX",
        "outputId": "3584315e-43e9-4289-9f9c-6925c7fb5a27"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of character: 3231\n",
            "Chapter 1 – The Boy Who Lived.\n",
            "Mr. and Mrs. Dursley, of number four, Privet\n",
            "Drive, were proud to sa\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "4q-jl3guUkeQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2. Tokenize the data\n",
        "The next step involves splitting our text into individual word-level tokens. We need to be careful with punctuation and spacing to ensure we correctly isolate each part of the text.\n",
        "- Import `re`, Python’s regular expression module.\n",
        "- Preprocess the text using `re.split` with a regular expression.  \n",
        "  A good starting pattern is:  \n",
        "  `[,.:;?_!\"()\\']|--|\\s`  \n",
        "  This splits text into words, punctuation, and whitespace.\n",
        "- Real-world text often contains additional characters or formatting, so you may need to adjust the regular expression or apply additional filtering for things like line breaks or tabs.\n",
        "- Use a list comprehension (preferred) or a loop to remove empty strings or unwanted tokens that were not handled by the regular expression, such as tabs or line breaks.\n",
        "\n",
        "<details>\n",
        "  <summary>Click Here to view solution</summary>\n",
        "\n",
        "```python\n",
        "import re\n",
        "\n",
        "# Creates tokens for words, punctuation, and whitespace\n",
        "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
        "\n",
        "# Filters out tabs and line breaks\n",
        "preprocessed = [\n",
        "    item for item in preprocessed\n",
        "    if item and item not in {\"\\n\", \"\\t\"}\n",
        "]\n",
        "\n",
        "print(preprocessed[:38])"
      ],
      "metadata": {
        "id": "aRRYyL8sk0mK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xAfaudnLXXQ3",
        "outputId": "cf52f968-53d5-4fb5-9ce0-ee49ca47d707"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Chapter', ' ', '1', '', '', 'I', ' ', 'am', ' ', 'by', ' ', 'birth', ' ', 'a', ' ', 'Genevese', ',', '', ' ', 'and', ' ', 'my', ' ', 'family', ' ', 'is', ' ', 'one', ' ', 'of', ' ', 'the', ' ', 'most', 'distinguished', ' ', 'of', ' ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3. Filter out duplicate words\n",
        "To build a vocabular for our LLM we want to get rid of any duplicates.\n",
        "- A `set` is a list-like data structure in Python that does not allow duplicates.\n",
        "- Converting a list to a set will automatically remove duplicate items.\n",
        "- Use Python’s `set()` to remove duplicates from `preprocessed`, then use `sorted()` to sort the resulting list.\n",
        "- Print the number of unique tokens in the vocabulary.\n",
        "\n",
        "<details>\n",
        "  <summary>Click Here to view solution</summary>\n",
        "\n",
        "```python\n",
        "all_words = sorted(set(preprocessed))\n",
        "vocab_size = len(all_words)\n",
        "\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "id": "-zFp3qZhp7Po"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJu6UhlYXr_Z",
        "outputId": "95912318-a7cc-4363-e891-d927c853b5b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2013\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4. Build vocab dictionary\n",
        "While we understand words naturally, our models do not. To work with text, we need to encode words into numerical values. We will start by creating a dictionary that assigns an index value to each token, which will act as its ID or encoded representation.\n",
        "- Use the `enumerate()` method on `all_words`. This will return a list of tokens and their indices.  \n",
        "  If we had `['cat', 'dog']`, this would become:  \n",
        "  `[(0, 'cat'), (1, 'dog')]`\n",
        "- Create a variable called `vocab` that is a dictionary created from this list, where:\n",
        "  - the key is the token\n",
        "  - the value is the integer  \n",
        "  Example: `{'cat': 0, 'dog': 1}`\n",
        "\n",
        "<details>\n",
        "  <summary>Click Here to view solution</summary>\n",
        "\n",
        "~~~python\n",
        "vocab = {token: integer for integer, token in enumerate(all_words)}\n",
        "~~~\n",
        "\n",
        "</details>"
      ],
      "metadata": {
        "id": "uVJZRQAVw3W_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UmZbrAHQXtHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4. Tokenizer Class\n",
        "Next we will need to make a class that can encode and decode our text.\n",
        "- Create a class called `SimpleTokenizerV1`.\n",
        "\n",
        "- Give the class two attributes:\n",
        "  - `str_to_int`: a dictionary mapping tokens to integer IDs.\n",
        "  - `int_to_str`: a reverse dictionary mapping integer IDs back to tokens.\n",
        "    - This is created using `{i: s for s, i in vocab.items()}`.\n",
        "\n",
        "- Create a method called `encode` that:\n",
        "  - Takes a string as input.\n",
        "  - Splits the text into tokens using punctuation and whitespace.\n",
        "  - Removes whitespace-only tokens (spaces, tabs, and line breaks).\n",
        "  - Converts each token into its corresponding integer ID.\n",
        "  - Returns a list of integers.\n",
        "\n",
        "- Create a method called `decode` that:\n",
        "  - Takes a list of integer IDs as input.\n",
        "  - Converts each ID back into its token.\n",
        "  - Joins tokens into a single string using spaces.\n",
        "  - Fixes spacing before punctuation.\n",
        "  - Returns the reconstructed text.\n",
        "\n",
        "<details>\n",
        "  <summary>Click Here to view solution</summary>\n",
        "\n",
        "```python\n",
        "import re\n",
        "\n",
        "class SimpleTokenizerV1:\n",
        "    def __init__(self, vocab):\n",
        "        self.str_to_int = vocab\n",
        "        self.int_to_str = {i: s for s, i in vocab.items()}\n",
        "    \n",
        "    def encode(self, text):\n",
        "        preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
        "        preprocessed = [\n",
        "            item.strip() for item in preprocessed if item.strip()\n",
        "        ]\n",
        "        ids = [self.str_to_int[s] for s in preprocessed]\n",
        "        return ids\n",
        "        \n",
        "    def decode(self, ids):\n",
        "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "        # Replace spaces before the specified punctuation\n",
        "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "        return text"
      ],
      "metadata": {
        "id": "dxVOnRh3sJk8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1jUFUvkkXxtW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5. Testing the Tokenizer Class\n",
        "\n",
        "Let’s make sure everything is working correctly.\n",
        "\n",
        "- Instantiate the tokenizer class using the `vocab` dictionary and store it in a variable called `tokenizer`.\n",
        "- Select a line of text from your source data.\n",
        "  - The text **must** come from the same source used to build the vocabulary.\n",
        "  - If the text contains words or symbols that are not in the vocabulary, encoding will fail.\n",
        "- Run `.encode()` on the text.\n",
        "- Print the encoded token IDs to verify the output.\n",
        "- Call and print decode on the Id's to assure it's correct.\n",
        "\n",
        "<details>\n",
        "  <summary>Click Here to view solution</summary>\n",
        "\n",
        "```python\n",
        "tokenizer = SimpleTokenizerV1(vocab)\n",
        "\n",
        "text = \"Before this I was not unacquainted with the more obvious laws of electricity.\"\n",
        "ids = tokenizer.encode(text)\n",
        "print(ids)"
      ],
      "metadata": {
        "id": "iUURIkaa2CK6"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0XnfQ3tMXziB",
        "outputId": "ae903e6d-c047-4e34-f746-d96813f60612"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[29, 1796, 66, 1918, 1248, 1849, 1956, 1775, 1199, 1262, 1068, 1268, 670, 6]\n",
            "Before this I was not unacquainted with the more obvious laws of electricity.\n",
            "Before this I was not unacquainted with the more obvious laws of electricity.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "a4ue8nKbX1mq",
        "outputId": "fe5f16c0-045a-4823-f3e5-f9a70e875876"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Before this I was not unacquainted with the more obvious laws of electricity.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 248
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "owxCCAHvX3Gl",
        "outputId": "858fdc4b-e2af-47df-ccaa-50c3ba3d7705"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Before this I was not unacquainted with the more obvious laws of electricity.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 249
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 6. Something a Bit More Complex\n",
        "\n",
        "We just walked through building our own simple tokenizer. Thankfully, there are existing tools that handle tokenization for us in a much more robust way.\n",
        "\n",
        "In this step, we will use **tiktoken**, the tokenizer used by GPT-style models. Unlike our word-level tokenizer, tiktoken breaks text into **subword tokens**, allowing it to handle unknown words more gracefully.\n",
        "\n",
        "- Create a new variable called `tokenizer` and set it using `tiktoken.get_encoding(\"gpt2\")`.\n",
        "- Create a string of text to encode (it can be any text).\n",
        "- Call `tokenizer.encode()` and pass in the text.\n",
        "- Print the resulting list of token IDs.\n",
        "\n",
        "<details>\n",
        "  <summary>Click Here to view solution</summary>\n",
        "\n",
        "```python\n",
        "import tiktoken\n",
        "\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "text = (\n",
        "    \"Hello, do you like tea? In the sunlit terraces \"\n",
        "    \"of someunknownPlace.\"\n",
        ")\n",
        "\n",
        "integers = tokenizer.encode(text)\n",
        "\n",
        "print(integers)\n"
      ],
      "metadata": {
        "id": "svFB9Wp55R9T"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ADrEkhv8X8GV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fc37sqNX-LN",
        "outputId": "1cbf7016-95e6-4bf1-ffd1-b1e6211a70e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[15496, 11, 466, 345, 588, 8887, 30, 220, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n",
            "[15496, 11, 466, 345, 588, 8887, 30, 220, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 7. Testing decoding\n",
        "Lest try it's decoding method\n",
        "- Call tokenizer.decode and pass it our intagers\n",
        "- print the string to assure it's correct\n",
        "<details>\n",
        "  <summary>Click Here to view solution</summary>\n",
        "\n",
        "```python\n",
        "strings = tokenizer.decode(integers)\n",
        "\n",
        "print(strings)"
      ],
      "metadata": {
        "id": "bAmiFTBs7PoK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHQhnkAAX_Ju",
        "outputId": "7ce639b0-4fd8-409f-b6c5-3920056f115d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, do you like tea?  In the sunlit terracesof someunknownPlace.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 8. Data Sampling\n",
        "Now that we have tokens we need to create loading. Our LLMs aren't just trained on individule words but the sequence in which those words appear. We need to create batches of words for the model. A sliding \"window\" of the words in sequental order.\n",
        "-  create_dataloader_v1 from supplementary\n",
        "- call create_dataloader witht he folowing arguments, raw_tet, batch_size=8, max_length=4, stride=4, shuffle=False\n",
        "  - raw_text was our data from the begging of this lab\n",
        "  - batch_size is how many sequencies\n",
        "  - max_length is the number of tokens in a sequence or the \"window\" size\n",
        "  - strice is how far the window slides forward each time\n",
        "  - shuffle=False keeps our sequences in order\n",
        "  - create a varaible called data_iter and set it to iter(dataloader)\n",
        "    - This allows us to move though the batches using next()\n",
        "  - create a variable called inputs and targets and set them to next(data_iter)\n",
        "  -print inputs and targets\n",
        "  <details>\n",
        "  <summary>Click Here to view solution</summary>\n",
        "\n",
        "    ```python\n",
        "    from supplementary import create_dataloader_v1\n",
        "\n",
        "\n",
        "    dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=4, stride=4, shuffle=False)\n",
        "\n",
        "    data_iter = iter(dataloader)\n",
        "    inputs, targets = next(data_iter)\n",
        "    print(\"Inputs:\\n\", inputs)\n",
        "    print(\"\\nTargets:\\n\", targets)\n",
        "    ```"
      ],
      "metadata": {
        "id": "eJvc6VtG8eT3"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VofSy0w9YD4s",
        "outputId": "9c8f36a7-8db1-46b9-9d98-2889df8b5edf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs:\n",
            " tensor([[14126,   352,   628,   198],\n",
            "        [   40,   716,   416,  4082],\n",
            "        [  257, 13005,  1158,    68],\n",
            "        [   11,   290,   616,  1641],\n",
            "        [  318,   530,   286,   262],\n",
            "        [  749,   198, 17080, 46709],\n",
            "        [  286,   326, 17146,    13],\n",
            "        [ 2011, 18668,   550,   587]])\n",
            "\n",
            "Targets:\n",
            " tensor([[  352,   628,   198,    40],\n",
            "        [  716,   416,  4082,   257],\n",
            "        [13005,  1158,    68,    11],\n",
            "        [  290,   616,  1641,   318],\n",
            "        [  530,   286,   262,   749],\n",
            "        [  198, 17080, 46709,   286],\n",
            "        [  326, 17146,    13,  2011],\n",
            "        [18668,   550,   587,   329]])\n"
          ]
        }
      ]
    }
  ]
}