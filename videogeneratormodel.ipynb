{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/florianaewing/CSB430SWIWinter2026/blob/main/videogeneratormodel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "c6892ec6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6892ec6",
        "outputId": "cbedd28a-f627-4141-f66b-8a9b199b7d94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "['.config', 'sample_data', '.ipynb_checkpoints']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "print(os.getcwd())\n",
        "print(os.listdir())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "3500fd36",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3500fd36",
        "outputId": "d0f7d3bb-8812-4062-b61b-1a1050fc7d12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 2795 frames, shape: (2795, 18, 32, 3)\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "video_path = \"sample_data/daytime_earth.mp4\"\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "frames = []\n",
        "frame_count = 0\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "    # Resize to 32x18 (16:9) for pixel-art\n",
        "    frame = cv2.resize(frame, (32, 18))\n",
        "    # Ensure RGB\n",
        "    if frame.shape[2] == 3:\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    elif frame.shape[2] == 4:  # remove alpha if present\n",
        "        frame = frame[:, :, :3]\n",
        "    # Normalize to [0,1]\n",
        "    frame = frame.astype(np.float32) / 255.0\n",
        "    frames.append(frame)\n",
        "    frame_count += 1\n",
        "\n",
        "cap.release()\n",
        "frames = np.array(frames)\n",
        "print(f\"Loaded {frame_count} frames, shape: {frames.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "debd4865",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "debd4865",
        "outputId": "4da630c2-f562-4131-bf91-79e55142b38a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created 2779 sequences, X shape: (2779, 16, 18, 32, 3), Y shape: (2779, 16, 18, 32, 3)\n",
            "Using 1000 sequences for training\n",
            "After transpose, X shape: (1000, 16, 3, 18, 32) Y shape: (1000, 16, 3, 18, 32)\n"
          ]
        }
      ],
      "source": [
        "seq_len = 16\n",
        "X, Y = [], []\n",
        "\n",
        "num_sequences = len(frames) - seq_len\n",
        "if num_sequences <= 0:\n",
        "    raise ValueError(f\"Not enough frames ({len(frames)}) for sequence length {seq_len}\")\n",
        "\n",
        "for i in range(num_sequences):\n",
        "    seq_x = frames[i:i+seq_len]\n",
        "    seq_y = frames[i+1:i+seq_len+1]\n",
        "    # Ensure sequences have correct shape\n",
        "    if seq_x.shape != (seq_len, 18, 32, 3) or seq_y.shape != (seq_len, 18, 32, 3):\n",
        "        continue\n",
        "    X.append(seq_x)\n",
        "    Y.append(seq_y)\n",
        "\n",
        "X = np.array(X)\n",
        "Y = np.array(Y)\n",
        "print(f\"Created {X.shape[0]} sequences, X shape: {X.shape}, Y shape: {Y.shape}\")\n",
        "\n",
        "max_sequences = 1000\n",
        "X = X[:max_sequences]\n",
        "Y = Y[:max_sequences]\n",
        "print(f\"Using {X.shape[0]} sequences for training\")\n",
        "\n",
        "# Transpose to PyTorch format: (B, T, C, H, W)\n",
        "X = np.transpose(X, (0, 1, 4, 2, 3))\n",
        "Y = np.transpose(Y, (0, 1, 4, 2, 3))\n",
        "print(\"After transpose, X shape:\", X.shape, \"Y shape:\", Y.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "e8e1e404",
      "metadata": {
        "id": "e8e1e404"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class ConvLSTMCell(nn.Module):\n",
        "    def __init__(self, input_channels, hidden_channels, kernel_size):\n",
        "        super().__init__()\n",
        "        padding = kernel_size // 2\n",
        "        self.conv = nn.Conv2d(input_channels + hidden_channels,\n",
        "                              4 * hidden_channels,\n",
        "                              kernel_size,\n",
        "                              padding=padding)\n",
        "        self.hidden_channels = hidden_channels\n",
        "\n",
        "    def forward(self, x, h, c):\n",
        "        combined = torch.cat([x, h], dim=1)\n",
        "        conv_out = self.conv(combined)\n",
        "        cc_i, cc_f, cc_o, cc_g = torch.chunk(conv_out, 4, dim=1)\n",
        "        i = torch.sigmoid(cc_i)\n",
        "        f = torch.sigmoid(cc_f)\n",
        "        o = torch.sigmoid(cc_o)\n",
        "        g = torch.tanh(cc_g)\n",
        "        c_next = f * c + i * g\n",
        "        h_next = o * torch.tanh(c_next)\n",
        "        return h_next, c_next\n",
        "\n",
        "class ConvLSTM(nn.Module):\n",
        "    def __init__(self, input_channels=3, hidden_channels=64, kernel_size=3):\n",
        "        super().__init__()\n",
        "        self.cell = ConvLSTMCell(input_channels, hidden_channels, kernel_size)\n",
        "        self.decoder = nn.Conv2d(hidden_channels, input_channels, kernel_size=3, padding=1)\n",
        "\n",
        "    def forward(self, input_seq):\n",
        "        B, T, C, H, W = input_seq.shape\n",
        "        h = torch.zeros(B, 64, H, W).to(input_seq.device)\n",
        "        c = torch.zeros(B, 64, H, W).to(input_seq.device)\n",
        "        outputs = []\n",
        "        for t in range(T):\n",
        "            h, c = self.cell(input_seq[:, t], h, c)\n",
        "            out = torch.sigmoid(self.decoder(h))\n",
        "            outputs.append(out)\n",
        "        return torch.stack(outputs, dim=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "9b5e2d22",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9b5e2d22",
        "outputId": "25823587-c14e-4974-fb79-1047bbe76e3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50, Loss: 0.006846\n",
            "Epoch 2/50, Loss: 0.005299\n",
            "Epoch 3/50, Loss: 0.005066\n",
            "Epoch 4/50, Loss: 0.004828\n",
            "Epoch 5/50, Loss: 0.004708\n",
            "Epoch 6/50, Loss: 0.004612\n",
            "Epoch 7/50, Loss: 0.004570\n",
            "Epoch 8/50, Loss: 0.004542\n",
            "Epoch 9/50, Loss: 0.004506\n",
            "Epoch 10/50, Loss: 0.004454\n",
            "Epoch 11/50, Loss: 0.004415\n",
            "Epoch 12/50, Loss: 0.004384\n",
            "Epoch 13/50, Loss: 0.004355\n",
            "Epoch 14/50, Loss: 0.004324\n",
            "Epoch 15/50, Loss: 0.004298\n",
            "Epoch 16/50, Loss: 0.004257\n",
            "Epoch 17/50, Loss: 0.004218\n",
            "Epoch 18/50, Loss: 0.004171\n",
            "Epoch 19/50, Loss: 0.004130\n",
            "Epoch 20/50, Loss: 0.004085\n",
            "Epoch 21/50, Loss: 0.004038\n",
            "Epoch 22/50, Loss: 0.004002\n",
            "Epoch 23/50, Loss: 0.003970\n",
            "Epoch 24/50, Loss: 0.003930\n",
            "Epoch 25/50, Loss: 0.003892\n",
            "Epoch 26/50, Loss: 0.003886\n",
            "Epoch 27/50, Loss: 0.003877\n",
            "Epoch 28/50, Loss: 0.003855\n",
            "Epoch 29/50, Loss: 0.003838\n",
            "Epoch 30/50, Loss: 0.003813\n",
            "Epoch 31/50, Loss: 0.003766\n",
            "Epoch 32/50, Loss: 0.003739\n",
            "Epoch 33/50, Loss: 0.003731\n",
            "Epoch 34/50, Loss: 0.003737\n",
            "Epoch 35/50, Loss: 0.003709\n",
            "Epoch 36/50, Loss: 0.003719\n",
            "Epoch 37/50, Loss: 0.003709\n",
            "Epoch 38/50, Loss: 0.003640\n",
            "Epoch 39/50, Loss: 0.003627\n",
            "Epoch 40/50, Loss: 0.003596\n",
            "Epoch 41/50, Loss: 0.003545\n",
            "Epoch 42/50, Loss: 0.003534\n",
            "Epoch 43/50, Loss: 0.003509\n",
            "Epoch 44/50, Loss: 0.003472\n",
            "Epoch 45/50, Loss: 0.003463\n",
            "Epoch 46/50, Loss: 0.003464\n",
            "Epoch 47/50, Loss: 0.003457\n",
            "Epoch 48/50, Loss: 0.003435\n",
            "Epoch 49/50, Loss: 0.003426\n",
            "Epoch 50/50, Loss: 0.003429\n"
          ]
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = ConvLSTM().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "X_tensor = torch.tensor(X, dtype=torch.float32).to(device)\n",
        "Y_tensor = torch.tensor(Y, dtype=torch.float32).to(device)\n",
        "\n",
        "batch_size = 4\n",
        "num_epochs = 50\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for i in range(0, len(X_tensor), batch_size):\n",
        "        x_batch = X_tensor[i:i+batch_size]\n",
        "        y_batch = Y_tensor[i:i+batch_size]\n",
        "        optimizer.zero_grad()\n",
        "        pred = model(x_batch)\n",
        "        loss = criterion(pred, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.6f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4223261",
      "metadata": {
        "id": "c4223261"
      },
      "source": [
        "# Real Time Display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20ac221d",
      "metadata": {
        "collapsed": true,
        "id": "20ac221d"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "seed = X_tensor[0:1]  # first sequence\n",
        "h = torch.zeros(1, 64, 18, 32).to(device)\n",
        "c = torch.zeros(1, 64, 18, 32).to(device)\n",
        "\n",
        "frame = seed[:, 0]\n",
        "\n",
        "scale = 20  # scale tiny frames for display\n",
        "\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow # Import cv2_imshow\n",
        "\n",
        "with torch.no_grad():\n",
        "    while True:  # keep generating until window closed\n",
        "        h, c = model.cell(frame, h, c)\n",
        "        frame = torch.sigmoid(model.decoder(h))\n",
        "        # Pixel-art quantization\n",
        "        frame_vis = (frame * 15).round() / 15\n",
        "        # Convert to HWC for OpenCV\n",
        "        img = frame_vis.cpu().numpy()[0].transpose(1, 2, 0)\n",
        "        img_up = cv2.resize(img, (32*scale, 18*scale), interpolation=cv2.INTER_NEAREST)\n",
        "        img_up = (img_up * 255).astype('uint8')[:, :, ::-1]  # RGB -> BGR\n",
        "\n",
        "        cv2_imshow(img_up) # Use cv2_imshow instead of cv2.imshow\n",
        "        if cv2.waitKey(30) & 0xFF == ord('q'):\n",
        "            break  # press 'q' to quit\n",
        "\n",
        "cv2.destroyAllWindows()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}